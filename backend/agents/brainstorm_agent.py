"""
åŒæ¨¡å‹å¤´è„‘é£æš´Agentå®ç°
"""
from typing import AsyncIterator, Dict, Any, List, Optional
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from .base_agent import BaseAgent, AgentMessage, AgentResponse
from backend.core.llm_pool import llm_pool, LLMConnection
from backend.core.logger import logger
import asyncio
import json


class BrainstormAgent(BaseAgent):
    """
    åŒæ¨¡å‹å¤´è„‘é£æš´Agent
    ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„AIæ¨¡å‹è¿›è¡Œåä½œè®¨è®ºï¼Œä¸ºç”¨æˆ·æä¾›å¤šè§’åº¦çš„æ€è€ƒå’Œæ´å¯Ÿ
    """

    def __init__(self, config: Dict[str, Any], agent_id: str = None):
        super().__init__(config)
        self.agent_id = agent_id
        self.config_dict = config
        self.current_connections: Dict[str, Optional[LLMConnection]] = {
            "model_a": None,
            "model_b": None
        }
        logger.info(f"BrainstormAgentåˆå§‹åŒ– - agent_id: {agent_id}")

    def update_config(self, new_config: Dict[str, Any]):
        """æ›´æ–°Agenté…ç½®"""
        logger.info(f"æ›´æ–°BrainstormAgenté…ç½® - agent_id: {self.agent_id}")
        self.config_dict = new_config
        # æ¸…é™¤å½“å‰è¿æ¥ï¼Œå¼ºåˆ¶é‡æ–°è·å–
        self.current_connections = {
            "model_a": None,
            "model_b": None
        }

    def _get_llm_connection(self, model_key: str) -> LLMConnection:
        """ä»è¿æ¥æ± è·å–æŒ‡å®šæ¨¡å‹çš„LLMè¿æ¥"""
        if not self.agent_id:
            raise ValueError("Agent IDä¸èƒ½ä¸ºç©º")

        # éªŒè¯æ¨¡å‹é…ç½®
        model_name_key = f"{model_key}_name"
        api_key_key = f"{model_key}_api_key"
        
        if not self.config_dict.get(model_name_key):
            raise ValueError(f"æ¨¡å‹{model_key}åç§°ä¸èƒ½ä¸ºç©º")
        
        if not self.config_dict.get(api_key_key):
            raise ValueError(f"æ¨¡å‹{model_key}çš„APIå¯†é’¥ä¸èƒ½ä¸ºç©º")

        # æ„å»ºæ¨¡å‹ç‰¹å®šé…ç½®
        model_config = {
            "model_name": self.config_dict[model_name_key],
            "temperature": self.config_dict.get(f"{model_key}_temperature", 0.7),
            "api_key": self.config_dict[api_key_key],
            "base_url": self.config_dict.get(f"{model_key}_base_url", "https://api.openai.com/v1"),
            "max_tokens": self.config_dict.get("max_tokens")
        }

        # ä»è¿æ¥æ± è·å–è¿æ¥
        connection = llm_pool.get_llm_connection(
            agent_id=f"{self.agent_id}_{model_key}",
            agent_type="brainstorm_agent",
            config=model_config
        )

        if not connection:
            raise RuntimeError(f"æ— æ³•ä»è¿æ¥æ± è·å–{model_key}çš„LLMè¿æ¥")

        self.current_connections[model_key] = connection
        logger.info(f"è·å–{model_key}è¿æ¥æˆåŠŸ - connection_id: {connection.connection_id}")
        return connection

    def _release_llm_connections(self):
        """é‡Šæ”¾æ‰€æœ‰LLMè¿æ¥å›è¿æ¥æ± """
        for model_key, connection in self.current_connections.items():
            if connection:
                llm_pool.release_llm_connection(connection.connection_id)
                logger.info(f"é‡Šæ”¾{model_key}è¿æ¥ - connection_id: {connection.connection_id}")
                self.current_connections[model_key] = None

    async def process_message(
        self,
        message: AgentMessage,
        context: Dict[str, Any]
    ) -> AsyncIterator[AgentResponse]:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›åŒæ¨¡å‹è®¨è®ºçš„æµå¼å“åº”
        """
        try:
            # è·å–åŒæ¨¡å‹è¿æ¥
            connection_a = self._get_llm_connection("model_a")
            connection_b = self._get_llm_connection("model_b")
            
            llm_a = connection_a.llm
            llm_b = connection_b.llm

            # è·å–é…ç½®å‚æ•°
            max_rounds = self.config_dict.get("max_discussion_rounds", 5)
            discussion_style = self.config_dict.get("discussion_style", "collaborative")
            enable_summary = self.config_dict.get("enable_summary", True)

            logger.info(f"å¼€å§‹åŒæ¨¡å‹è®¨è®º - topic: {message.content[:50]}...")

            # å‘é€è®¨è®ºå¼€å§‹ä¿¡å·
            yield AgentResponse(
                content=f"ğŸ¯ **è®¨è®ºä¸»é¢˜**: {message.content}\n\n",
                is_final=False,
                metadata={
                    "discussion_phase": "start",
                    "model_a": self.config_dict.get("model_a_name"),
                    "model_b": self.config_dict.get("model_b_name"),
                    "max_rounds": max_rounds,
                    "style": discussion_style
                }
            )

            # è¿›è¡Œå¤šè½®è®¨è®º
            discussion_history = []
            for round_num in range(1, max_rounds + 1):
                logger.info(f"å¼€å§‹ç¬¬{round_num}è½®è®¨è®º")
                
                # æ¨¡å‹Aå‘è¨€
                yield AgentResponse(
                    content=f"## ğŸ¤– æ¨¡å‹A ({self.config_dict.get('model_a_name')}) - ç¬¬{round_num}è½®\n\n",
                    is_final=False,
                    metadata={"discussion_phase": "model_a_start", "round": round_num}
                )
                
                full_response_a = ""
                async for chunk in self._get_model_response(
                    llm_a, message.content, discussion_history, "model_a", round_num, discussion_style
                ):
                    full_response_a += chunk
                    yield AgentResponse(
                        content=chunk,
                        is_final=False,
                        metadata={"discussion_phase": "model_a_speaking", "round": round_num}
                    )
                
                # è®°å½•æ¨¡å‹Açš„å®Œæ•´å“åº”
                discussion_history.append({
                    "round": round_num,
                    "speaker": "model_a",
                    "content": full_response_a
                })

                yield AgentResponse(content="\n\n", is_final=False)

                # æ¨¡å‹Bå‘è¨€
                yield AgentResponse(
                    content=f"## ğŸ¤– æ¨¡å‹B ({self.config_dict.get('model_b_name')}) - ç¬¬{round_num}è½®\n\n",
                    is_final=False,
                    metadata={"discussion_phase": "model_b_start", "round": round_num}
                )
                
                full_response_b = ""
                async for chunk in self._get_model_response(
                    llm_b, message.content, discussion_history, "model_b", round_num, discussion_style
                ):
                    full_response_b += chunk
                    yield AgentResponse(
                        content=chunk,
                        is_final=False,
                        metadata={"discussion_phase": "model_b_speaking", "round": round_num}
                    )
                
                # è®°å½•æ¨¡å‹Bçš„å®Œæ•´å“åº”
                discussion_history.append({
                    "round": round_num,
                    "speaker": "model_b", 
                    "content": full_response_b
                })

                yield AgentResponse(content="\n\n---\n\n", is_final=False)

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥æå‰ç»“æŸè®¨è®º
                if await self._should_end_discussion(discussion_history, round_num):
                    logger.info(f"è®¨è®ºåœ¨ç¬¬{round_num}è½®åè‡ªç„¶ç»“æŸ")
                    break

            # ç”Ÿæˆè®¨è®ºæ€»ç»“
            summary_content = ""
            if enable_summary:
                yield AgentResponse(
                    content="## è®¨è®ºæ€»ç»“\n\n",
                    is_final=False,
                    metadata={"discussion_phase": "summary_start"}
                )

                async for chunk in self._generate_summary(llm_a, message.content, discussion_history):
                    summary_content += chunk
                    yield AgentResponse(
                        content=chunk,
                        is_final=False,
                        metadata={"discussion_phase": "summary"}
                    )

            # å‘é€æœ€ç»ˆå“åº”æ ‡è®°
            yield AgentResponse(
                content="",
                is_final=True,
                metadata={
                    "discussion_phase": "complete",
                    "total_rounds": len(discussion_history) // 2,
                    "discussion_history": discussion_history,
                    "summary_content": summary_content  # æ·»åŠ å®Œæ•´çš„æ€»ç»“å†…å®¹
                }
            )

            logger.info("åŒæ¨¡å‹è®¨è®ºå®Œæˆ")

        except Exception as e:
            logger.error(f"åŒæ¨¡å‹è®¨è®ºå¤±è´¥ - agent_id: {self.agent_id}, é”™è¯¯: {e}")
            yield AgentResponse(
                content=f"æŠ±æ­‰ï¼Œè®¨è®ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                is_final=True,
                metadata={"error": str(e)}
            )
        finally:
            # ç¡®ä¿é‡Šæ”¾æ‰€æœ‰è¿æ¥
            self._release_llm_connections()

    async def _get_model_response(
        self,
        llm: ChatOpenAI,
        topic: str,
        history: List[Dict],
        model_role: str,
        round_num: int,
        style: str
    ) -> AsyncIterator[str]:
        """è·å–æŒ‡å®šæ¨¡å‹çš„å“åº”"""
        import time

        messages = self._build_discussion_messages(topic, history, model_role, round_num, style)

        # è®°å½•å¼€å§‹æ—¶é—´å’ŒåŸºæœ¬ä¿¡æ¯
        start_time = time.time()
        total_length = 0
        chunk_count = 0

        logger.info(f"ğŸ¤– å¼€å§‹è°ƒç”¨LLM - æ¨¡å‹: {model_role}, è½®æ¬¡: {round_num}, "
                   f"agent_id: {self.agent_id}, æ¶ˆæ¯æ•°: {len(messages)}")

        async for chunk in llm.astream(messages):
            if chunk.content:
                chunk_count += 1
                total_length += len(chunk.content)

                # æ¯10ä¸ªchunkæˆ–æ¯100ä¸ªå­—ç¬¦è®°å½•ä¸€æ¬¡è¿›åº¦
                if chunk_count % 10 == 0 or total_length % 100 == 0:
                    elapsed = time.time() - start_time
                    logger.info(f"ğŸ“ LLMå“åº”è¿›åº¦ - æ¨¡å‹: {model_role}, è½®æ¬¡: {round_num}, "
                               f"chunkæ•°: {chunk_count}, ç´¯è®¡é•¿åº¦: {total_length}, "
                               f"è€—æ—¶: {elapsed:.2f}s")

                yield chunk.content

        # è®°å½•å®Œæˆä¿¡æ¯
        total_time = time.time() - start_time
        logger.info(f"âœ… LLMå“åº”å®Œæˆ - æ¨¡å‹: {model_role}, è½®æ¬¡: {round_num}, "
                   f"æ€»chunkæ•°: {chunk_count}, æ€»é•¿åº¦: {total_length}, "
                   f"æ€»è€—æ—¶: {total_time:.2f}s, å¹³å‡é€Ÿåº¦: {total_length/total_time:.1f}å­—ç¬¦/ç§’")

    def _build_discussion_messages(
        self,
        topic: str,
        history: List[Dict],
        model_role: str,
        round_num: int,
        style: str
    ) -> List:
        """æ„å»ºè®¨è®ºæ¶ˆæ¯åˆ—è¡¨"""
        messages = []

        # è·å–æ¨¡å‹ç‰¹å®šçš„ç³»ç»Ÿæç¤º
        system_prompt_key = f"{model_role}_system_prompt"
        custom_prompt = self.config_dict.get(system_prompt_key)

        if custom_prompt:
            system_prompt = custom_prompt
        else:
            # é»˜è®¤ç³»ç»Ÿæç¤º
            if style == "collaborative":
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„è®¨è®ºå‚ä¸è€…ï¼Œæ­£åœ¨ä¸å¦ä¸€ä¸ªAIæ¨¡å‹å°±ç‰¹å®šè¯é¢˜è¿›è¡Œåä½œå¼è®¨è®ºã€‚

è®¨è®ºè§„åˆ™ï¼š
1. ä¿æŒå»ºè®¾æ€§å’Œåä½œçš„æ€åº¦
2. åŸºäºå¯¹æ–¹çš„è§‚ç‚¹è¿›è¡Œæ·±å…¥æ€è€ƒå’Œè¡¥å……
3. æä¾›æ–°çš„è§†è§’å’Œè§è§£
4. é¿å…ç®€å•é‡å¤ï¼Œè¦æœ‰åˆ›æ–°æ€§æ€è€ƒ
5. ä¿æŒé€»è¾‘æ¸…æ™°ï¼Œè®ºè¯æœ‰åŠ›

å½“å‰æ˜¯ç¬¬{round_num}è½®è®¨è®ºï¼Œè¯·é’ˆå¯¹è¯é¢˜æä¾›ä½ çš„è§‚ç‚¹å’Œåˆ†æã€‚"""
            else:  # debate
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå–„äºè¾©è®ºçš„æ™ºèƒ½å‚ä¸è€…ï¼Œæ­£åœ¨ä¸å¦ä¸€ä¸ªAIæ¨¡å‹å°±ç‰¹å®šè¯é¢˜è¿›è¡Œè¾©è®ºå¼è®¨è®ºã€‚

è¾©è®ºè§„åˆ™ï¼š
1. å¯ä»¥è´¨ç–‘å¯¹æ–¹çš„è§‚ç‚¹ï¼Œä½†è¦ä¿æŒå°Šé‡
2. æä¾›æœ‰åŠ›çš„åé©³è®ºæ®å’Œè¯æ®
3. åšæŒè‡ªå·±çš„ç«‹åœºï¼Œä½†ä¹Ÿè¦æ‰¿è®¤åˆç†çš„è§‚ç‚¹
4. é€»è¾‘ä¸¥å¯†ï¼Œé¿å…äººèº«æ”»å‡»
5. è¿½æ±‚çœŸç†ï¼Œè€Œéå•çº¯çš„èƒœè´Ÿ

å½“å‰æ˜¯ç¬¬{round_num}è½®è¾©è®ºï¼Œè¯·é’ˆå¯¹è¯é¢˜å’Œå¯¹æ–¹è§‚ç‚¹è¿›è¡Œå›åº”ã€‚"""

        messages.append(SystemMessage(content=system_prompt))

        # æ·»åŠ è®¨è®ºå†å²
        for item in history:
            speaker = "æ¨¡å‹A" if item["speaker"] == "model_a" else "æ¨¡å‹B"
            if item["speaker"] == model_role:
                messages.append(AIMessage(content=f"[ç¬¬{item['round']}è½®] {item['content']}"))
            else:
                messages.append(HumanMessage(content=f"[ç¬¬{item['round']}è½® {speaker}] {item['content']}"))

        # æ·»åŠ å½“å‰è®¨è®ºä¸»é¢˜
        if round_num == 1:
            messages.append(HumanMessage(content=f"è®¨è®ºä¸»é¢˜ï¼š{topic}"))
        else:
            messages.append(HumanMessage(content=f"è¯·ç»§ç»­å°±ã€Œ{topic}ã€è¿™ä¸ªä¸»é¢˜è¿›è¡Œç¬¬{round_num}è½®è®¨è®º"))

        return messages

    async def _should_end_discussion(self, history: List[Dict], current_round: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æå‰ç»“æŸè®¨è®º"""
        # ç®€å•çš„ç»“æŸæ¡ä»¶ï¼šå¦‚æœè¿ç»­ä¸¤è½®è®¨è®ºå†…å®¹ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œåˆ™ç»“æŸ
        if len(history) >= 4:  # è‡³å°‘éœ€è¦ä¸¤è½®å®Œæ•´è®¨è®º
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„ç›¸ä¼¼åº¦æ£€æµ‹é€»è¾‘
            # æš‚æ—¶è¿”å›Falseï¼Œè®©è®¨è®ºè¿›è¡Œåˆ°æœ€å¤§è½®æ•°
            pass
        return False

    async def _generate_summary(
        self,
        llm: ChatOpenAI,
        topic: str,
        history: List[Dict]
    ) -> AsyncIterator[str]:
        """ç”Ÿæˆè®¨è®ºæ€»ç»“"""
        import time

        summary_prompt = self.config_dict.get("summary_prompt") or """è¯·å¯¹ä»¥ä¸‹è®¨è®ºè¿›è¡Œæ€»ç»“ï¼š

1. æ€»ç»“åŒæ–¹çš„ä¸»è¦è§‚ç‚¹
2. æŒ‡å‡ºè®¨è®ºä¸­çš„å…±è¯†å’Œåˆ†æ­§
3. æä¾›ç»¼åˆæ€§çš„ç»“è®ºå’Œå»ºè®®
4. æŒ‡å‡ºå¯èƒ½çš„åç»­è®¨è®ºæ–¹å‘

è¯·ä¿æŒå®¢è§‚å’Œå…¨é¢ã€‚"""

        messages = [
            SystemMessage(content=summary_prompt),
            HumanMessage(content=f"è®¨è®ºä¸»é¢˜ï¼š{topic}\n\nè®¨è®ºå†…å®¹ï¼š\n" +
                        "\n".join([f"ç¬¬{item['round']}è½® {'æ¨¡å‹A' if item['speaker'] == 'model_a' else 'æ¨¡å‹B'}ï¼š{item['content']}"
                                 for item in history]))
        ]

        # è®°å½•å¼€å§‹æ—¶é—´å’ŒåŸºæœ¬ä¿¡æ¯
        start_time = time.time()
        total_length = 0
        chunk_count = 0

        logger.info(f"ğŸ“ å¼€å§‹ç”Ÿæˆè®¨è®ºæ€»ç»“ - agent_id: {self.agent_id}, "
                   f"è®¨è®ºè½®æ•°: {len(history)//2}, å†å²é•¿åº¦: {sum(len(item['content']) for item in history)}")

        async for chunk in llm.astream(messages):
            if chunk.content:
                chunk_count += 1
                total_length += len(chunk.content)

                # æ¯5ä¸ªchunkè®°å½•ä¸€æ¬¡è¿›åº¦
                if chunk_count % 5 == 0:
                    elapsed = time.time() - start_time
                    logger.info(f"ğŸ“„ æ€»ç»“ç”Ÿæˆè¿›åº¦ - chunkæ•°: {chunk_count}, "
                               f"ç´¯è®¡é•¿åº¦: {total_length}, è€—æ—¶: {elapsed:.2f}s")

                yield chunk.content

        # è®°å½•å®Œæˆä¿¡æ¯
        total_time = time.time() - start_time
        logger.info(f"âœ… è®¨è®ºæ€»ç»“å®Œæˆ - æ€»chunkæ•°: {chunk_count}, æ€»é•¿åº¦: {total_length}, "
                   f"æ€»è€—æ—¶: {total_time:.2f}s")

    def get_config_schema(self) -> Dict[str, Any]:
        """è¿”å›é…ç½®æ¨¡å¼å®šä¹‰"""
        return {
            "type": "object",
            "properties": {
                # æ¨¡å‹Aé…ç½®
                "model_a_name": {
                    "type": "string",
                    "default": "gemini-2.5-flash-preview-05-20",
                    "description": "æ¨¡å‹Açš„åç§°"
                },
                "model_a_temperature": {
                    "type": "number",
                    "default": 0.7,
                    "minimum": 0.0,
                    "maximum": 2.0,
                    "description": "æ¨¡å‹Açš„éšæœºæ€§æ§åˆ¶"
                },
                "model_a_api_key": {
                    "type": "string",
                    "description": "æ¨¡å‹Açš„APIå¯†é’¥"
                },
                "model_a_base_url": {
                    "type": "string",
                    "default": "https://api.openai.com/v1",
                    "description": "æ¨¡å‹Açš„APIåŸºç¡€URL"
                },
                "model_a_system_prompt": {
                    "type": "string",
                    "description": "æ¨¡å‹Açš„ç³»ç»Ÿæç¤ºè¯"
                },

                # æ¨¡å‹Bé…ç½®
                "model_b_name": {
                    "type": "string",
                    "default": "claude-3-sonnet",
                    "description": "æ¨¡å‹Bçš„åç§°"
                },
                "model_b_temperature": {
                    "type": "number",
                    "default": 0.7,
                    "minimum": 0.0,
                    "maximum": 2.0,
                    "description": "æ¨¡å‹Bçš„éšæœºæ€§æ§åˆ¶"
                },
                "model_b_api_key": {
                    "type": "string",
                    "description": "æ¨¡å‹Bçš„APIå¯†é’¥"
                },
                "model_b_base_url": {
                    "type": "string",
                    "default": "https://api.anthropic.com/v1",
                    "description": "æ¨¡å‹Bçš„APIåŸºç¡€URL"
                },
                "model_b_system_prompt": {
                    "type": "string",
                    "description": "æ¨¡å‹Bçš„ç³»ç»Ÿæç¤ºè¯"
                },

                # è®¨è®ºé…ç½®
                "max_discussion_rounds": {
                    "type": "integer",
                    "default": 5,
                    "minimum": 1,
                    "maximum": 10,
                    "description": "æœ€å¤§è®¨è®ºè½®æ•°"
                },
                "discussion_style": {
                    "type": "string",
                    "enum": ["collaborative", "debate"],
                    "default": "collaborative",
                    "description": "è®¨è®ºé£æ ¼ï¼šåä½œå¼æˆ–è¾©è®ºå¼"
                },
                "enable_summary": {
                    "type": "boolean",
                    "default": True,
                    "description": "æ˜¯å¦å¯ç”¨è®¨è®ºæ€»ç»“"
                },
                "summary_prompt": {
                    "type": "string",
                    "description": "æ€»ç»“æç¤ºè¯ï¼ˆå¯é€‰ï¼‰"
                }
            },
            "required": [
                "model_a_name", "model_a_api_key",
                "model_b_name", "model_b_api_key"
            ]
        }

    def validate_config(self, config: Dict[str, Any]) -> bool:
        """éªŒè¯é…ç½®å‚æ•°"""
        required_fields = [
            "model_a_name", "model_a_api_key",
            "model_b_name", "model_b_api_key"
        ]

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if field not in config or not config[field]:
                return False

        # æ£€æŸ¥temperatureèŒƒå›´
        for temp_field in ["model_a_temperature", "model_b_temperature"]:
            if temp_field in config:
                temp = config[temp_field]
                if not isinstance(temp, (int, float)) or temp < 0 or temp > 2:
                    return False

        # æ£€æŸ¥è®¨è®ºè½®æ•°
        if "max_discussion_rounds" in config:
            rounds = config["max_discussion_rounds"]
            if not isinstance(rounds, int) or rounds < 1 or rounds > 10:
                return False

        # æ£€æŸ¥è®¨è®ºé£æ ¼
        if "discussion_style" in config:
            style = config["discussion_style"]
            if style not in ["collaborative", "debate"]:
                return False

        return True

    def get_supported_features(self) -> List[str]:
        """è¿”å›æ”¯æŒçš„åŠŸèƒ½ç‰¹æ€§"""
        return [
            "dual_model_discussion",
            "streaming_response",
            "configurable_models",
            "discussion_summary",
            "collaborative_mode",
            "debate_mode"
        ]
