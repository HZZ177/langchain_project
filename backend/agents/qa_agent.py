"""
é—®ç­”Agentå®ç°
"""
from typing import AsyncIterator, Dict, Any, List, Optional
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from .base_agent import BaseAgent, AgentMessage, AgentResponse
from backend.core.llm_pool import llm_pool, LLMConnection
from backend.core.logger import logger


class QAAgent(BaseAgent):
    """
    å¤§æ¨¡å‹é—®ç­”Agent
    æä¾›é€šç”¨çš„AIé—®ç­”åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§é—®é¢˜ç±»å‹
    """

    def __init__(self, config: Dict[str, Any], agent_id: str = None):
        super().__init__(config)
        self.agent_id = agent_id
        self.config_dict = config
        self.current_connection: Optional[LLMConnection] = None
        logger.info(f"QAAgentåˆå§‹åŒ– - agent_id: {agent_id}")
    
    def _get_llm_connection(self) -> LLMConnection:
        """ä»è¿æ¥æ± è·å–LLMè¿æ¥"""
        if not self.agent_id:
            raise ValueError("Agent IDä¸èƒ½ä¸ºç©º")

        # éªŒè¯å¿…è¦çš„é…ç½®
        if not self.config.api_key:
            raise ValueError("APIå¯†é’¥ä¸èƒ½ä¸ºç©º")

        if not self.config.model_name:
            raise ValueError("æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º")

        # ä»è¿æ¥æ± è·å–è¿æ¥
        connection = llm_pool.get_llm_connection(
            agent_id=self.agent_id,
            agent_type="qa_agent",
            config=self.config_dict
        )

        if not connection:
            raise RuntimeError(f"æ— æ³•ä»è¿æ¥æ± è·å–LLMè¿æ¥ - agent_id: {self.agent_id}")

        self.current_connection = connection
        logger.info(f"è·å–LLMè¿æ¥æˆåŠŸ - connection_id: {connection.connection_id}")
        return connection

    def _release_llm_connection(self):
        """é‡Šæ”¾LLMè¿æ¥å›è¿æ¥æ± """
        if self.current_connection:
            llm_pool.release_llm_connection(self.current_connection.connection_id)
            logger.info(f"é‡Šæ”¾LLMè¿æ¥ - connection_id: {self.current_connection.connection_id}")
            self.current_connection = None
    
    async def process_message(
        self,
        message: AgentMessage,
        context: Dict[str, Any]
    ) -> AsyncIterator[AgentResponse]:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›æµå¼å“åº”
        """
        connection = None
        try:
            # ä»è¿æ¥æ± è·å–LLMè¿æ¥
            connection = self._get_llm_connection()
            llm = connection.llm

            # æ„å»ºæ¶ˆæ¯å†å²
            messages = self._build_messages(message, context)

            logger.info(f"å¼€å§‹å¤„ç†æ¶ˆæ¯ - connection_id: {connection.connection_id}, "
                       f"agent_id: {self.agent_id}, message_length: {len(message.content)}")

            # æµå¼è°ƒç”¨LLM
            import time
            start_time = time.time()
            full_response = ""
            chunk_count = 0

            logger.info(f"ğŸ¤– å¼€å§‹è°ƒç”¨LLM - æ¨¡å‹: {self.config.model_name}, "
                       f"agent_id: {self.agent_id}, connection_id: {connection.connection_id}")

            async for chunk in llm.astream(messages):
                if chunk.content:
                    chunk_count += 1
                    full_response += chunk.content

                    # æ¯20ä¸ªchunkè®°å½•ä¸€æ¬¡è¿›åº¦
                    if chunk_count % 20 == 0:
                        elapsed = time.time() - start_time
                        logger.info(f"ğŸ“ LLMå“åº”è¿›åº¦ - chunkæ•°: {chunk_count}, "
                                   f"ç´¯è®¡é•¿åº¦: {len(full_response)}, è€—æ—¶: {elapsed:.2f}s")

                    yield AgentResponse(
                        content=chunk.content,
                        is_final=False,
                        metadata={
                            "model": self.config.model_name,
                            "temperature": self.config.temperature,
                            "connection_id": connection.connection_id
                        }
                    )

            # è®°å½•å®Œæˆä¿¡æ¯
            total_time = time.time() - start_time
            logger.info(f"âœ… LLMå“åº”å®Œæˆ - æ¨¡å‹: {self.config.model_name}, "
                       f"æ€»chunkæ•°: {chunk_count}, æ€»é•¿åº¦: {len(full_response)}, "
                       f"æ€»è€—æ—¶: {total_time:.2f}s, å¹³å‡é€Ÿåº¦: {len(full_response)/total_time:.1f}å­—ç¬¦/ç§’")

            # å‘é€æœ€ç»ˆå“åº”æ ‡è®°
            yield AgentResponse(
                content="",
                is_final=True,
                metadata={
                    "model": self.config.model_name,
                    "total_tokens": len(full_response.split()),
                    "full_response": full_response,
                    "connection_id": connection.connection_id,
                    "usage_count": connection.usage_count,
                    "total_time": total_time,
                    "chunk_count": chunk_count
                }
            )

            logger.info(f"æ¶ˆæ¯å¤„ç†å®Œæˆ - connection_id: {connection.connection_id}, "
                       f"response_length: {len(full_response)}")

        except Exception as e:
            logger.error(f"å¤„ç†æ¶ˆæ¯å¤±è´¥ - agent_id: {self.agent_id}, é”™è¯¯: {e}")
            yield AgentResponse(
                content=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                is_final=True,
                metadata={"error": str(e)}
            )
        finally:
            # ç¡®ä¿é‡Šæ”¾è¿æ¥
            if connection:
                self._release_llm_connection()
    
    def _build_messages(self, message: AgentMessage, context: Dict[str, Any]) -> List:
        """æ„å»ºLangChainæ¶ˆæ¯åˆ—è¡¨"""
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿè¿›è¡Œè‡ªç„¶çš„ä¸­æ–‡å¯¹è¯ã€‚è¯·æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚"
        messages.append(SystemMessage(content=system_prompt))
        
        # æ·»åŠ å†å²å¯¹è¯
        conversations = context.get("conversations", [])
        max_rounds = getattr(self.config, 'max_conversation_rounds', 5)  # ä»é…ç½®è·å–è½®æ•°ï¼Œé»˜è®¤5è½®
        for conv in conversations[-max_rounds:]:  # ä¿ç•™æœ€è¿‘Nè½®å¯¹è¯
            if conv["message_type"] == "user":
                messages.append(HumanMessage(content=conv["content"]))
            elif conv["message_type"] == "assistant":
                messages.append(AIMessage(content=conv["content"]))
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append(HumanMessage(content=message.content))
        
        return messages
    
    def get_config_schema(self) -> Dict[str, Any]:
        """è¿”å›é…ç½®æ¨¡å¼å®šä¹‰"""
        return {
            "type": "object",
            "properties": {
                "model_name": {
                    "type": "string",
                    "default": "gemini-2.5-flash-preview-05-20",
                    "description": "ä½¿ç”¨çš„æ¨¡å‹åç§°"
                },
                "temperature": {
                    "type": "number",
                    "default": 0.7,
                    "minimum": 0.0,
                    "maximum": 2.0,
                    "description": "æ§åˆ¶å›ç­”çš„éšæœºæ€§"
                },
                "max_tokens": {
                    "type": ["integer", "null"],
                    "default": None,
                    "minimum": 1,
                    "description": "æœ€å¤§ç”Ÿæˆtokenæ•°ï¼Œç•™ç©ºè¡¨ç¤ºæ— é™åˆ¶"
                },
                "api_key": {
                    "type": "string",
                    "description": "OpenAI APIå¯†é’¥"
                },
                "base_url": {
                    "type": "string",
                    "default": "https://api.openai.com/v1",
                    "description": "APIåŸºç¡€URL"
                }
            },
            "required": ["model_name", "api_key"]
        }
    
    def validate_config(self, config: Dict[str, Any]) -> bool:
        """éªŒè¯é…ç½®å‚æ•°"""
        required_fields = ["model_name", "api_key"]
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if field not in config or not config[field]:
                return False
        
        # æ£€æŸ¥temperatureèŒƒå›´
        if "temperature" in config:
            temp = config["temperature"]
            if not isinstance(temp, (int, float)) or temp < 0 or temp > 2:
                return False
        
        # æ£€æŸ¥max_tokensèŒƒå›´
        if "max_tokens" in config:
            max_tokens = config["max_tokens"]
            if max_tokens is not None:
                if not isinstance(max_tokens, int) or max_tokens < 1:
                    return False
        
        return True
    
    def get_supported_features(self) -> List[str]:
        """è¿”å›æ”¯æŒçš„åŠŸèƒ½ç‰¹æ€§"""
        return [
            "basic_chat",
            "streaming_response",
            "conversation_history",
            "configurable_model"
        ]
